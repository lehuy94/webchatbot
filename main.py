# main.py

import os
import google.generativeai as genai
import streamlit as st
from dotenv import load_dotenv

# Load environment variables from .env file
# This is crucial for securely loading your API key.
load_dotenv()

# --- Configuration ---
# Get your Google API Key from environment variables.
# It's recommended to set GOOGLE_API_KEY in your system's environment
# or in a .env file in the root of your project.

API_KEY = os.getenv("GOOGLE_API_KEY")
#API_KEY = os.environ.get("GEMINI_API_KEY", "")

if not API_KEY:
    st.error("Lá»—i: KhÃ´ng tÃ¬m tháº¥y GOOGLE_API_KEY. Vui lÃ²ng Ä‘áº·t nÃ³ lÃ m biáº¿n mÃ´i trÆ°á»ng hoáº·c trong tá»‡p .env.")
    st.info("Báº¡n cÃ³ thá»ƒ láº¥y API Key tá»« Google AI Studio (aistudio.google.com).")
    st.stop() # Stop the Streamlit app if API key is missing

# Configure the generative AI model
genai.configure(api_key=API_KEY)

# Initialize the Gemini model (cached for performance)
# We use 'gemini-pro' for text generation.
# Adjust generation configuration for desired output.
@st.cache_resource
def get_gemini_model():
    """Caches the Gemini GenerativeModel instance."""
    return genai.GenerativeModel(
        'gemini-2.0-flash',
        generation_config={
            "temperature": 0.7,  # Controls randomness. Lower for more deterministic output.
            "max_output_tokens": 1024, # Maximum number of tokens in the response.
        }
    )

model = get_gemini_model()

# --- Helper Functions ---

def read_file_content(uploaded_file):
    """
    Reads the content of an uploaded text file from Streamlit.
    Args:
        uploaded_file (streamlit.runtime.uploaded_file_manager.UploadedFile): The file uploaded via Streamlit.
    Returns:
        str: The content of the file, or None if an error occurs.
    """
    try:
        # Decode the bytes content of the uploaded file to a string
        content = uploaded_file.read().decode('utf-8')
        return content
    except Exception as e:
        st.error(f"Lá»—i khi Ä‘á»c tá»‡p: {e}. Vui lÃ²ng Ä‘áº£m báº£o Ä‘Ã¢y lÃ  tá»‡p vÄƒn báº£n há»£p lá»‡.")
        return None

def get_chat_response(file_content, user_query):
    """
    Interacts with the Gemini model using file content as context.
    Args:
        file_content (str): The content of the file to use as context.
        user_query (str): The user's question.
    Returns:
        str: The model's response.
    """
    if not file_content:
        return "TÃ´i khÃ´ng cÃ³ ná»™i dung tá»‡p Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i cá»§a báº¡n."

    # Construct the prompt by combining file content and user's query.
    # It's important to clearly delineate the context and the question.
    prompt = f"""
    Báº¡n lÃ  má»™t trá»£ lÃ½ chatbot thÃ´ng minh. Báº¡n sáº½ tráº£ lá»i cÃ¡c cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng dá»±a trÃªn Ná»˜I DUNG Tá»†P sau Ä‘Ã¢y.
    Náº¿u cÃ¢u há»i khÃ´ng liÃªn quan Ä‘áº¿n ná»™i dung tá»‡p, hÃ£y nÃ³i ráº±ng báº¡n chá»‰ cÃ³ thá»ƒ tráº£ lá»i cÃ¡c cÃ¢u há»i liÃªn quan Ä‘áº¿n tá»‡p Ä‘Ã£ cung cáº¥p.

    Ná»˜I DUNG Tá»†P:
    ---
    {file_content}
    ---

    CÃ‚U Há»I Cá»¦A NGÆ¯á»œI DÃ™NG: {user_query}

    TRáº¢ Lá»œI Cá»¦A Báº N:
    """
    try:
        with st.spinner("Äang suy nghÄ©..."): # Show a loading spinner while waiting for the API response
            response = model.generate_content(prompt)
            return response.text
    except Exception as e:
        st.error(f"Lá»—i khi gá»i API Gemini: {e}")
        return "Xin lá»—i, tÃ´i gáº·p sá»± cá»‘ khi xá»­ lÃ½ yÃªu cáº§u cá»§a báº¡n. Vui lÃ²ng thá»­ láº¡i."

# --- Streamlit UI ---

def main():
    """
    Main function to run the Streamlit chatbot application.
    """
    st.set_page_config(page_title="Chatbot Ä‘á»c tá»‡p (Gemini AI)", page_icon="ğŸ“„", layout="centered")
    st.title("ğŸ“„ Chatbot")
    st.markdown("""
        ChÃ o má»«ng báº¡n Ä‘áº¿n vá»›i Chatbot Ä‘á»c tá»‡p!
        Táº£i lÃªn má»™t tá»‡p vÄƒn báº£n vÃ  há»i tÃ´i báº¥t cá»© Ä‘iá»u gÃ¬ liÃªn quan Ä‘áº¿n ná»™i dung cá»§a nÃ³.
    """)

    # File uploader widget
    uploaded_file = st.file_uploader("Chá»n má»™t tá»‡p vÄƒn báº£n (.txt) Ä‘á»ƒ táº£i lÃªn", type=["txt"])

    file_content = None
    if uploaded_file is not None:
        file_content = read_file_content(uploaded_file)
        if file_content:
            st.success("ÄÃ£ táº£i tá»‡p thÃ nh cÃ´ng! BÃ¢y giá» báº¡n cÃ³ thá»ƒ há»i cÃ¡c cÃ¢u há»i.")
            # Display a snippet of the file content for confirmation
            with st.expander("Xem ná»™i dung tá»‡p Ä‘Ã£ táº£i (100 kÃ½ tá»± Ä‘áº§u)"):
                st.code(file_content[:500] + "..." if len(file_content) > 500 else file_content)
        else:
            uploaded_file = None # Reset if file reading failed

    # Chat input and history
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # Display chat messages from history on app rerun
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # React to user input
    if prompt := st.chat_input("Báº¡n há»i gÃ¬ vá» ná»™i dung tá»‡p?"):
        # Display user message in chat message container
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # Get response from chatbot
        if uploaded_file is None:
            response_text = "Vui lÃ²ng táº£i lÃªn má»™t tá»‡p trÆ°á»›c khi Ä‘áº·t cÃ¢u há»i."
        else:
            response_text = get_chat_response(file_content, prompt)
        
        # Display assistant response in chat message container
        st.session_state.messages.append({"role": "assistant", "content": response_text})
        with st.chat_message("assistant"):
            st.markdown(response_text)

if __name__ == "__main__":
    main()
